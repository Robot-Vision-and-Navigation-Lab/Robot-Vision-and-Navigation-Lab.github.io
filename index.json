[{"authors":null,"categories":null,"content":"I was a Professor in the Department of Computing Science at the University of Alberta. I received my BSc degree from Northeastern University (USA) and my PhD from Purdue University. After a year of post-doctoral training at the University of Pennsylvania, I joined the U of A in 1988 where I currently hold an adjunct appointment.\nMy research interests include robotics, computer vision, and image processing. I have worked in a number of areas in robotics and, for the past 10+ years, my focus has been on visual robot navigation. In 2003-17, with support from the federal and provinncial goverments, I held an NSERC Industrial Research Chair to conduct research in computer vision and image processing that addresses the practical challenges facing Alberta’s mining industry. As a member of the NSERC Strategic Network on Robotics (NCRN) I work closely with Canadian academic colleagues and industrial partners on mobile robotics research. Within the international robotics community, my activities include a variety of roles in ICRA and IROS communities, as well as other IEEE RAS sponsored conferences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9ba420b8ac562a35c57569aeb0c48f01","permalink":"https://robot-vision-and-navigation-lab.github.io/author/hong-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hong-zhang/","section":"authors","summary":"I was a Professor in the Department of Computing Science at the University of Alberta. I received my BSc degree from Northeastern University (USA) and my PhD from Purdue University. After a year of post-doctoral training at the University of Pennsylvania, I joined the U of A in 1988 where I currently hold an adjunct appointment.","tags":null,"title":"Hong Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3dd3edd38efbd636db31f9e083b88ff6","permalink":"https://robot-vision-and-navigation-lab.github.io/author/li-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-he/","section":"authors","summary":"","tags":null,"title":"Li He","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"94c29cab9a33bd653548dfb123e435fd","permalink":"https://robot-vision-and-navigation-lab.github.io/author/weinan-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weinan-chen/","section":"authors","summary":"","tags":null,"title":"Weinan Chen","type":"authors"},{"authors":["Chen","Weinan and Zhu","Lei and Loo","Shing Yan and Wang","Jiankun and Wang","Chaoqun and Meng","Max Q.-H. and Zhang","Hong"],"categories":null,"content":"","date":1651643061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651643061,"objectID":"a448c779be61c5e88efeed327bb3b4f9","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/robust-improvement-in-3d-object-landmark-inference-for-semantic-mapping/","publishdate":"2022-05-04T05:44:21.731Z","relpermalink":"/publication/robust-improvement-in-3d-object-landmark-inference-for-semantic-mapping/","section":"publication","summary":"Recent works on semantic Simultaneous Localization and Mapping (SLAM) utilizing object landmarks have shown superiority in terms of robustness and accuracy in tracking and localization. 3D object landmarks represented by a cubic or quadric surface are inferred from 2D object bounding boxes which are typically captured from multiple views by an object detector. Nevertheless, bounding box noises and small camera baseline may lead to an inaccurate 3D object landmark inference. Inspired by the dual quadric enveloping property, in this work, we introduce the horizontal support assumption to constrain rotation w.r.t. roll and pitch for a quadric representation. As the result, we reduce the number of quadric parameters and narrow down the solution space, and ultimately produce a relatively accurate inference. Extensive experimental evaluations under both simulated and real scenarios are conducted in this paper. Quantitative results demonstrate that our approach outperforms the state-of-the-art.","tags":null,"title":"Robust Improvement in 3D Object Landmark Inference for Semantic Mapping","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Lin","Xubin and He","Li and Guan","Yisheng and Zhang","Hong"],"categories":null,"content":"","date":1651642992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651642992,"objectID":"bb6271cdfc29ba1afa5a460fc903475c","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/dynamic-strategy-of-keyframe-selection-with-pd-controller-for-vslam-systems/","publishdate":"2022-05-04T05:43:12.823Z","relpermalink":"/publication/dynamic-strategy-of-keyframe-selection-with-pd-controller-for-vslam-systems/","section":"publication","summary":"Keyframe (KF) selection in a KF-based visual simultaneous localization and mapping (VSLAM) system is critical. In previous studies, static thresholds have been used for KF selection decision making; however, suboptimal performance can result from the use of such thresholds. To obtain a better KF setting than that obtained with the existing methods, in this article, we introduce a dynamic KF selection strategy. By considering both the view change between camera observation and KFs in the built map and the rate of this change, we propose to dynamically adjust the threshold for KF selection. A proportion and derivative (PD) controller is designed with the feedback of estimated view change, where the PD controller output is used for KF selection. According to the experimental results, compared with the existing studies, our method can improve the precision of visual tracking by 17.5% and 16.7% based on two popular VSLAM systems.","tags":null,"title":"Dynamic Strategy of Keyframe Selection With PD Controller for VSLAM Systems","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Wang","Chaoqun and He","Li and Meng","Max Q.-H."],"categories":null,"content":"","date":1651642853,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651642853,"objectID":"0c93708c0db59caa9d2e8941bd654993","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/ceb-map-visual-localization-error-prediction-for-safe-navigation/","publishdate":"2022-05-04T05:40:53.255Z","relpermalink":"/publication/ceb-map-visual-localization-error-prediction-for-safe-navigation/","section":"publication","summary":"For safe visual navigation, areas with high localization errors should be concentrated and could be further reﬁned by additional mapping operations. Given an environment map, we propose to predict the visual localization error and hence to either improve the navigation performance or call an additional mapping to reﬁne the built map. Previous work adopts the uncertainty of landmarks for the error prediction. In our work, we take into account both the spatial distribution of visual landmarks and the uncertainty of landmarks. Our main idea is that standing at one place, a good spatial distribution of landmarks means a sufﬁcient enough visible landmarks from all views at that place, i.e., enough landmarks under arbitrary view-direction. Combining the spatial distribution and the uncertainty of landmarks, we propose a new framework to predict the error of visual localization. Furthermore, we show that additional mapping in the area with high predicted error can signiﬁcantly improve the visual localization precision. Experimental results show that there is a strong relationship between the predicted error and the real error, of which the absolute value of correlation coefﬁcient is between 0.707 to 0.915. We apply our method to conduct an optimal reﬁning policy on the built map and the experimental results show the improved localization precision. Applications on navigation test verify the superiority of our proposed method.","tags":null,"title":"CEB-Map: Visual Localization Error Prediction for Safe Navigation","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Loo","Shing Yan and Wang","Jiankun and Wang","Chaoqun and Meng","Max Q.-H. and Zhang","Hong"],"categories":null,"content":"","date":1651635574,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651635574,"objectID":"1cb0d6b27efa71de5bc87cf736c4c9e6","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/robustness-improvement-of-using-pre-trained-network-in-visual-odometry-for-on-road-driving/","publishdate":"2022-05-04T03:39:34.551Z","relpermalink":"/publication/robustness-improvement-of-using-pre-trained-network-in-visual-odometry-for-on-road-driving/","section":"publication","summary":"Robustness in on-road driving Visual Odometry (VO) systems is critical, as it determines the reliable performance in various scenarios and environments. Especially with the development of data-driven technology, the combination of data-driven VO and model-based VO has achieved accurate tracking performance. However, the lack of generalization of pre-trained deep neural networks (DNN) limits the robustness of such a combination in unseen environments. In this study, we introduce a novel framework with appropriate usage of DNN prediction and improve the robustness in the self-driving application. Based on the characteristic of on-road self-driving motion and the DNN output, we propose a two-step optimization strategy with a variable degree of freedom (DoF), i.e., the use of two types of DoF representations during pose estimation. Speciﬁcally, our two-step optimization operates according to the residual of the optimization with the motion label classiﬁcation from the pre-trained DNN, as well as our proposed Motion Evaluation by essential matrix construction. Experimental results show that our framework obtains better tracking accuracy than the existing methods.","tags":null,"title":"Robustness Improvement of Using Pre-Trained Network in Visual Odometry for On-Road Driving","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://robot-vision-and-navigation-lab.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://robot-vision-and-navigation-lab.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5067e3c181b3e0650396f57d3552e7e3","permalink":"https://robot-vision-and-navigation-lab.github.io/research_highlights/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_highlights/","section":"","summary":"","tags":null,"title":"Tour","type":"widget_page"}]